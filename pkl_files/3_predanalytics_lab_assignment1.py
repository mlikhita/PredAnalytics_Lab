# -*- coding: utf-8 -*-
"""3_PredAnalytics_Lab_Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KWQu_R4C0bLJFFVZdAGzai_vZz7TsyyW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import regex as re
import pickle

#### for linear regression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler, OneHotEncoder

#### for svc
from sklearn.metrics import confusion_matrix, classification_report
from sklearn import svm

### for random forest
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

#from google.colab import drive
#drive.mount('/content/drive')

data_df = pd.read_csv('laptop_price.csv', encoding='latin-1')

data_df.head()

data_df.info()

# Replace 'GB' with '' in 'Ram' column and convert to float
data_df['Ram'] = data_df['Ram'].str.replace('GB', '').astype(float)

# Replace 'kg' with '' in 'Weight' column and convert to float
data_df['Weight'] = data_df['Weight'].str.replace('kg', '').astype(float)



# Exchange rate (as of today, you might want to update it or fetch it dynamically)
exchange_rate_eur_to_inr = 89.5  # 1 EUR = 89.5 INR (approximate)
# Function to convert EUR to INR
def eur_to_inr(euros):
  return euros * exchange_rate_eur_to_inr

# Convert 'Price_euros' to 'Price_inr'
data_df['PriceInr'] = data_df['Price_euros'].apply(eur_to_inr)
data_df = data_df.drop(columns=['Price_euros'])
data_df[['Ram','Weight','PriceInr']].head()

### Memory Column
data_df['Memory'] = data_df['Memory'].str.split(' ').str[0]

# Extract the memory size and unit
data_df['Memory_Size'] = data_df['Memory'].str.extract(r'(\d+)')
data_df['Memory_Unit'] = data_df['Memory'].str.extract(r'([a-zA-Z]+)')

# Convert 'Memory_Size' to numeric
data_df['Memory_Size'] = pd.to_numeric(data_df['Memory_Size'])

# Convert memory to GB
def convert_to_gb(row):
  size = row['Memory_Size']
  unit = row['Memory_Unit']
  if unit == 'GB':
    return size
  elif unit == 'TB':
    return size * 1024
  else:
      return 0
data_df['MemoryGB'] = data_df.apply(convert_to_gb, axis=1)
data_df = data_df.drop(columns=['Memory','Memory_Size', 'Memory_Unit'])
data_df['MemoryGB'].head()

#create a copy
mod_data_df = data_df.copy()
resolution_pattern = r'(\d+)x(\d+)'
screentype_pattern = r'^(.*?)(?=\s+\d{3,4}x\d{3,4})|^.*?(?<!\d{3,4}x\d{3,4})$'

# Extract resolution
mod_data_df['Resolution'] = mod_data_df['ScreenResolution'].str.extract(resolution_pattern).apply(lambda row: 'x'.join(row.astype(str)), axis=1)
# Extract screen type
texts = mod_data_df['ScreenResolution'].to_list()
mod_data_df['ScreenType'] = [re.match(screentype_pattern, text).group(1) if re.match(screentype_pattern, text) else text for text in texts]
mod_data_df['ScreenType'].head()

# Create TouchScreen column
mod_data_df['TouchScreen'] = np.where(mod_data_df['ScreenType'].str.contains('Touchscreen'), 1, 0)
mod_data_df['ScreenType']  = mod_data_df['ScreenType'].str.replace('Touchscreen / ','')
mod_data_df['ScreenType']  = mod_data_df['ScreenType'].str.replace(' / Touchscreen','')
mod_data_df['ScreenType']  = mod_data_df['ScreenType'].str.replace(' Touchscreen','')
mod_data_df['TouchScreen'] = mod_data_df['TouchScreen'].astype('category')
mod_data_df[['ScreenType','TouchScreen']].loc[100:105]

# Split CPU into name and speed
cpu_split = mod_data_df['Cpu'].str.extract(r'^(?P<ProcessorName>.+?)\s(?P<Speed>[0-9.]+GHz)$')
mod_data_df['ProcessorName'] = cpu_split[['ProcessorName']]
mod_data_df['ProcessorSpeed'] = cpu_split[['Speed']]

cpu_pattern = r'^(?P<Brand>Intel|AMD|Celeron|Pentium|Xeon|Ryzen|Core|Athlon)\s*(?P<Series>[A-Za-z0-9\-]+)\s*(?P<CoreType>(Core|Quad Core|Dual Core|M|E-Series|A-Series|Pentium|Ryzen|Celeron|FX|Athlon))?\s*(?P<Model>[A-Za-z0-9\-]+)\s*(?P<Generation>[A-Za-z0-9\-]+)?\s*(?P<Variant>[v0-9]+)?$'

# Example usage with various processor names
processors = mod_data_df['ProcessorName'].to_list()
processors = [str(text) for text in processors]
mod_data_df['CpuModel'] = [re.match(cpu_pattern, text).group('Brand') + " "+ re.match(cpu_pattern, text).group('Model') if re.match(cpu_pattern, text) else text for text in processors]
mod_data_df[['Cpu','CpuModel']].head()

# Create Gpu_brand column
mod_data_df['GpuBrand'] = mod_data_df['Gpu'].str.split(' ').str[0] + ' '+ mod_data_df['Gpu'].str.split(' ').str[1]
mod_data_df[['Gpu','GpuBrand']].head()

req_cols = ['Company','TypeName','Inches','Ram','OpSys','Weight','MemoryGB','ScreenType','TouchScreen','CpuModel','GpuBrand','PriceInr']
req_data_df = mod_data_df[req_cols]

filtered_data_df = req_data_df.copy()
# Compile lower frequencies into 'other'

cat_cols = list(filtered_data_df.select_dtypes(include=['object', 'category']).columns)

for col in cat_cols:
  counts = filtered_data_df[col].value_counts()
  threshold = 10  # Define a threshold for frequency. Can be adjusted.
  low_freq_categories = counts[counts < threshold].index
  filtered_data_df[col] = filtered_data_df[col].apply(lambda x: 'Other' if x in low_freq_categories else x)

x_cols = ['Company','TypeName','Inches','Ram','OpSys','Weight','MemoryGB','ScreenType','TouchScreen','CpuModel','GpuBrand']
for i in x_cols:
  print(f"{i} : {filtered_data_df[i].unique()}")

# Create quantile-based bins
filtered_data_df['Price_inr_QuantileBin'] = pd.qcut(filtered_data_df['PriceInr'], q=12)
filtered_data_df['price_class'] = pd.Categorical(filtered_data_df['Price_inr_QuantileBin']).codes
price_class_mapping = dict(zip(filtered_data_df['price_class'], filtered_data_df['Price_inr_QuantileBin']))
final_data_df = filtered_data_df.copy()
final_data_df['price_class'] = final_data_df['price_class'].astype('category')

price_class_mapping

req_columns = ['Company','TypeName','Inches','Ram','OpSys','Weight','MemoryGB','ScreenType','TouchScreen','CpuModel','GpuBrand','PriceInr']
final_data_df_2 = final_data_df[req_columns]

# Identify categorical columns
cat_cols = list(final_data_df_2.select_dtypes(include=['object', 'category']).columns)

# Dummy encode categorical columns
final_data_df_2 = pd.get_dummies(final_data_df_2, columns=cat_cols, drop_first=False)

final_data_df_2.info()

final_data_df_2.head()

from sklearn.model_selection import train_test_split

# Assuming 'final_data_df_1' is your preprocessed DataFrame
X = final_data_df_2.drop('PriceInr', axis=1)  # Features
y = final_data_df_2[['PriceInr']]  # Target variable
# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # 80% train, 20% test

print("Training data shape:", X_train.shape, y_train.shape)
print("Testing data shape:", X_test.shape, y_test.shape)

# Identify numerical columns
x_numerical_cols = X_train.select_dtypes(include=np.number).columns
y_numerical_cols = y_train.select_dtypes(include=np.number).columns

# Initialize StandardScaler
scaler_x = StandardScaler()
scaler_y = StandardScaler()

# Fit and transform numerical columns in X_train
X_train[x_numerical_cols] = scaler_x.fit_transform(X_train[x_numerical_cols])

# Transform numerical columns in X_test using the same scaler
X_test[x_numerical_cols] = scaler_x.transform(X_test[x_numerical_cols])

# Fit and transform numerical columns in X_train
y_train[y_numerical_cols] = scaler_y.fit_transform(y_train[y_numerical_cols])

# Fit and transform numerical columns in X_train
y_test[y_numerical_cols] = scaler_y.transform(y_test[y_numerical_cols])

x_numerical_cols

# Hyperparameter tuning using RandomizedSearchCV
rf_param_dist = {
    'n_estimators': [101, 201, 301, 401],
    'max_depth': [10, 20, 30, 40, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), rf_param_dist, n_iter=50, cv=5, n_jobs=-1)
rf_random_search.fit(X_train, y_train)
best_rf_model_2 = rf_random_search.best_estimator_
print("Best Parameters: ", rf_random_search.best_params_)

# Train and evaluate Random Forest Regressor
y_pred_rf = best_rf_model_2.predict(X_test)
print("Random Forest Regressor Mean Absolute Error: ", mean_absolute_error(y_test, y_pred_rf))
print("Random Forest Regressor Mean Squared Error: ", mean_squared_error(y_test, y_pred_rf))
print("Random Forest Regressor RÂ² Score: ", r2_score(y_test, y_pred_rf))

scaler_file_x = 'scaler_x.pkl'
scaler_file_y = 'scaler_y.pkl'
model_file_final = 'model_final.pkl'
model_input_schema = 'model_input_schema.pkl'

model_input_df = final_data_df_2.drop(columns=['PriceInr'])
column_dtype_mapping = {col: str(model_input_df[col].dtype) for col in model_input_df.columns}

# Save schema to a file
with open(model_input_schema, 'wb') as model_ip_schema_file:
    pickle.dump(column_dtype_mapping, model_ip_schema_file)

# Save the scaler as a pickle file
with open(scaler_file_x, 'wb') as scaler_x_file:
    pickle.dump(scaler_x, scaler_x_file)

# Save the scaler as a pickle file
with open(scaler_file_x, 'wb') as scaler_x_file:
    pickle.dump(scaler_x, scaler_x_file)

# Save the scaler as a pickle file
with open(scaler_file_y, 'wb') as scaler_y_file:
    pickle.dump(scaler_y, scaler_y_file)

# Save the model as a pickle file
with open(model_file_final, 'wb') as model_file:
    pickle.dump(best_rf_model_2, model_file)

# Company : ['Apple' 'HP' 'Acer' 'Asus' 'Dell' 'Lenovo' 'Other' 'MSI' 'Toshiba']
# TypeName : ['Ultrabook' 'Notebook' 'Netbook' 'Gaming' '2 in 1 Convertible'
#  'Workstation']
# OpSys : ['macOS' 'No OS' 'Windows 10' 'Other' 'Linux' 'Chrome OS' 'Windows 7']
# ScreenType : ['IPS Panel Retina Display' 'Other' 'Full HD' '1366x768'
#  'IPS Panel Full HD' 'Quad HD+' 'IPS Panel' 'Touchscreen'
#  'IPS Panel 4K Ultra HD' '4K Ultra HD' '1600x900' 'IPS Panel Quad HD+']
# TouchScreen : [0, 1]
# Categories (2, int64): [0, 1]
# cpu_model : ['Intel i5' 'Intel i7' 'AMD 9420' 'Intel i3' 'Other' 'Intel N3350'
#  'Intel N4200' 'Intel N3060' 'Intel N3710' 'Intel N3050']
# Gpu_brand : ['Intel Iris' 'Intel HD' 'AMD Radeon' 'Nvidia GeForce' 'Intel UHD' 'Other'
#  'Nvidia Quadro']

user_input = [13.3 , 256, 1.3, 512, 'Apple', 'Notebook','macOS','Full HD','Yes','Intel i7','AMD Radeon']
value_name = ['Inches','Ram','Weight','MemoryGB','Company','TypeName','OpSys','ScreenType','TouchScreen','CpuModel','GpuBrand']
ref_user_input = dict(zip(value_name, user_input))

print(ref_user_input)

scaler_file_x = 'scaler_x.pkl'
scaler_file_y = 'scaler_y.pkl'
model_file_final = 'model_final.pkl'
model_input_schema = 'model_input_schema.pkl'
scaler_x = pickle.load(open(scaler_file_x, 'rb'))
scaler_y = pickle.load(open(scaler_file_y, 'rb'))
model = pickle.load(open(model_file_final, 'rb'))
model_ip_schema = pickle.load(open(model_input_schema, 'rb'))

def build_model_inout(ref_user_input, scaler_x, model_ip_schema):
  user_value_names = ref_user_input.keys()
  model_input_cols = model_ip_schema.keys()
  model_input = {}

  for i in user_value_names:
    if i in model_input_cols: ### covers the case of integet values
      model_input[i] = ref_user_input[i]
      # model_input.append(ref_user_input[i])
    else:
      paticular_category =[x for x in model_input_cols if i in x]
      for j in paticular_category:
        if 'TouchScreen' in j:
          flag = 0 if ref_user_input[i] == 'No' else 1
          model_input[j] = 1 if str(flag) in j else 0
          # model_input.append(0 if ref_user_input[i] == 'No' else 1)
        elif j.split("_")[1]==ref_user_input[i]:
          model_input[j] = True
          # model_input.append(True)
        else:
          model_input[j] = False
          # model_input.append(False)
  model_input_df = pd.DataFrame(data = [model_input.values()], columns = model_input.keys())
  model_input_df = model_input_df.astype(model_ip_schema)
  x_numerical_cols = ['Inches','Ram','Weight','MemoryGB']
  model_input_df[x_numerical_cols] = scaler_x.transform(model_input_df[x_numerical_cols])
  return model_input_df

model_input_df = build_model_inout(ref_user_input, scaler_x, model_ip_schema)
model_input_df

y_pred_rf = model.predict(model_input_df)
result = scaler_y.inverse_transform(y_pred_rf.reshape(-1,1))[0][0]
print(result)